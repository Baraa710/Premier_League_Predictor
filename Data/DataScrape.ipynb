{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baraa710/Premier_League_Predictor/blob/main/DataScrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BQc1eNX3nK_",
        "outputId": "21ecb1c3-4e53-4942-c14b-8127990a8d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.7 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,641 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,920 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [61.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,107 kB]\n",
            "Fetched 8,447 kB in 4s (2,370 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package firefox-esr is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mPackage 'firefox-esr' has no installation candidate\u001b[0m\n",
            "--2024-04-07 01:50:07--  https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/c74e12d7-7166-4aaa-9d7a-bbb01471db75?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240407T015008Z&X-Amz-Expires=300&X-Amz-Signature=893ee87ca5fdb33d81210a572f0af9c38403253b7eee8ad278107c1cd234fd78&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.34.0-linux64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-04-07 01:50:08--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/c74e12d7-7166-4aaa-9d7a-bbb01471db75?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240407T015008Z&X-Amz-Expires=300&X-Amz-Signature=893ee87ca5fdb33d81210a572f0af9c38403253b7eee8ad278107c1cd234fd78&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.34.0-linux64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3249164 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘geckodriver-v0.34.0-linux64.tar.gz’\n",
            "\n",
            "geckodriver-v0.34.0 100%[===================>]   3.10M  12.8MB/s    in 0.2s    \n",
            "\n",
            "2024-04-07 01:50:09 (12.8 MB/s) - ‘geckodriver-v0.34.0-linux64.tar.gz’ saved [3249164/3249164]\n",
            "\n",
            "geckodriver\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# prompt: code to use firefox webdriver in google colab\n",
        "\n",
        "!apt-get update\n",
        "!apt install firefox-esr\n",
        "!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n",
        "!tar -xzvf geckodriver-v0.34.0-linux64.tar.gz\n",
        "!mv geckodriver /usr/local/bin\n",
        "!pip install selenium\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myby2Arghm82"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import multiprocessing\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHgOeeRTL1Cs",
        "outputId": "43fba161-9245-4568-a30f-61b8a1680183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1EVKr5yYAW0"
      },
      "outputs": [],
      "source": [
        "ranges = [(n, n+1) for n in range(2006, 2009)]\n",
        "\n",
        "seasons = [\"{}-{}\".format(year1,year2) for (year1, year2) in ranges]\n",
        "stat_types = [('playingtime','stats_playing_time')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIvq_3qhmU7B",
        "outputId": "34d05076-2d89-42b8-f6f0-6207497d558d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/159.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m122.9/159.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzj4UOliUUjt"
      },
      "outputs": [],
      "source": [
        "def get_database(season, stat_type, table_id):\n",
        "  print(\"starting to get season {}\".format(season))\n",
        "  driver.get(\"https://fbref.com/en/comps/9/{}/{}/{}-Premier-League-Stats\".format(season,stat_type, season))\n",
        "  # Wait for the page to load dynamically\n",
        "  driver.implicitly_wait(5)\n",
        "\n",
        "  # Get the page source\n",
        "  html_data = driver.page_source\n",
        "\n",
        "  # Parse the HTML data using BeautifulSoup\n",
        "  soup = BeautifulSoup(html_data, \"html.parser\")\n",
        "  if not soup:\n",
        "    print(\"No soup found\")\n",
        "    return\n",
        "  # Find the table with id \"stats_standard\"\n",
        "  table = soup.find('table', {'id': table_id})\n",
        "  if table == None:\n",
        "    print(\"Could not find table for season {} stat type {}\".format(season, stat_type))\n",
        "    return\n",
        "  print(\"found table\\n\")\n",
        "  # Extract all rows from the table\n",
        "  rows = table.find_all('tr')\n",
        "  col_names = rows[1].find_all('th')\n",
        "  col_names = [ele.text.strip() for ele in col_names]\n",
        "  # Create an empty list to store the data\n",
        "  data = []\n",
        "\n",
        "  # Loop through the rows and extract the data from each row, skipping the first row\n",
        "  for row in rows[2:]:\n",
        "      cols = row.find_all('th') + row.find_all('td')\n",
        "      cols = [ele.text.strip() for ele in cols]\n",
        "      if(cols[0].isdigit()!=True):\n",
        "        continue\n",
        "      data.append(cols)  # Get rid of empty values\n",
        "\n",
        "  # Create a DataFrame from the data list\n",
        "  df = pd.DataFrame(data, columns = col_names)\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Player_data/{}.xlsx'.format(season)\n",
        "  if not os.path.isfile(filename):\n",
        "    # Create a new file\n",
        "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "        df.to_excel(writer, sheet_name=table_id)\n",
        "  else:\n",
        "    with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "      df.to_excel(writer, sheet_name=table_id)\n",
        "  print(\"finished processing season {} stat {}\".format(season, stat_type))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_season(season):\n",
        "  print(\"processing season {}\".format(season))\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    print(\"processing season {} stat {}\".format(season, stat_type))\n",
        "    get_database(season, stat_type, table_id)\n",
        "\n",
        "# Initialize driver\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--disable-gpu\")\n",
        "# Disable images\n",
        "options.set_preference(\"permissions.default.image\", 2)\n",
        "\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "# for season in seasons:\n",
        "#   process_season(season)\n",
        "\n",
        "# driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_season('2022-2023')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzIPd8EsRHJ2",
        "outputId": "feb3879b-49e3-455d-94de-96d40a182b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing season 2022-2023\n",
            "processing season 2022-2023 stat playingtime\n",
            "starting to get season 2022-2023\n",
            "found table\n",
            "\n",
            "finished processing season 2022-2023 stat playingtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyGX8sawLk1V",
        "outputId": "4277dedc-867d-4773-9ec8-b1f4d517bf24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2006-2007',\n",
              " '2007-2008',\n",
              " '2008-2009',\n",
              " '2009-2010',\n",
              " '2010-2011',\n",
              " '2011-2012',\n",
              " '2012-2013',\n",
              " '2013-2014',\n",
              " '2014-2015',\n",
              " '2015-2016',\n",
              " '2016-2017',\n",
              " '2017-2018',\n",
              " '2018-2019',\n",
              " '2019-2020',\n",
              " '2020-2021',\n",
              " '2021-2022',\n",
              " '2022-2023']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ranges = [(n, n+1) for n in range(2006, 2023)]\n",
        "seasons = [\"{}-{}\".format(year1,year2) for (year1, year2) in ranges]\n",
        "seasons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtHbLd7HZNmG",
        "outputId": "930d0a8f-8885-4ec1-d919-008b557fe2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing season 2009-2010\n",
            "processing season 2006-2007processing season 2009-2010 stat playingtime\n",
            "processing season 2006-2007 stat playingtime\n",
            "\n",
            "processing season 2010-2011\n",
            "processing season 2010-2011 stat playingtime\n",
            "processing season 2007-2008\n",
            "processing season 2007-2008 stat playingtime\n",
            "processing season 2011-2012\n",
            "processing season 2011-2012 stat playingtime\n",
            "processing season 2008-2009\n",
            "processing season 2008-2009 stat playingtime\n",
            "processing season 2012-2013\n",
            "processing season 2012-2013 stat playingtime\n",
            "processing season 2015-2016\n",
            "processing season 2015-2016 stat playingtime\n",
            "processing season 2013-2014\n",
            "processing season 2013-2014 stat playingtime\n",
            "processing season 2016-2017\n",
            "processing season 2016-2017 stat playingtime\n",
            "processing season 2014-2015\n",
            "processing season 2014-2015 stat playingtime\n",
            "processing season 2017-2018\n",
            "processing season 2017-2018 stat playingtime\n",
            "processing season 2018-2019\n",
            "processing season 2018-2019 stat playingtime\n",
            "processing season 2021-2022\n",
            "processing season 2021-2022 stat playingtime\n",
            "processing season 2019-2020\n",
            "processing season 2019-2020 stat playingtime\n",
            "processing season 2022-2023\n",
            "processing season 2022-2023 stat playingtime\n",
            "processing season 2020-2021\n",
            "processing season 2020-2021 stat playingtime\n"
          ]
        }
      ],
      "source": [
        "def get_squad_data(season, stat_type):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/10/{}/{}/{}-Championship-Stats\".format(season,stat_type, season)\n",
        "\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "  if not soup:\n",
        "    print(\"No soup found\")\n",
        "  # Extract the first two tables\n",
        "  tables = soup.find_all('table')\n",
        "\n",
        "  if not tables:\n",
        "    print(\"No table found for season {}\".format(season))\n",
        "    return\n",
        "  # Create a Pandas DataFrame for each table\n",
        "  dfs = [pd.read_html(str(table))[0] for table in tables]\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data_2/{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  if not os.path.isfile(filename):\n",
        "    # Create a new file\n",
        "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "        dfs[0].to_excel(writer, sheet_name=stat_type)\n",
        "  with pd.ExcelWriter(filename, mode='a', if_sheet_exists='replace', engine='openpyxl') as writer:\n",
        "          dfs[0].to_excel(writer, sheet_name='EFL Squad Data {}'.format(stat_type))\n",
        "\n",
        "def process_season(season):\n",
        "  print(\"processing season {}\".format(season))\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    print(\"processing season {} stat {}\".format(season, stat_type))\n",
        "    get_squad_data(season, stat_type)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_squad_data('2008-2009', 'stats')"
      ],
      "metadata": {
        "id": "1o6Zi7PEr1Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "  print(\"processing season {} stat {}\".format('2014-2015', stat_type))\n",
        "  get_squad_data('2014-2015', stat_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRjlRvOIPR8z",
        "outputId": "9ab705fc-a951-4e64-a01b-1b5184f4d380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing season 2014-2015 stat stats\n",
            "processing season 2014-2015 stat keepers\n",
            "processing season 2014-2015 stat shooting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "z5KoHr3xai4f"
      },
      "outputs": [],
      "source": [
        "ranges = [(n, n+1) for n in range(2006, 2024)]\n",
        "\n",
        "seasons = [\"{}-{}\".format(year1,year2) for (year1, year2) in ranges]\n",
        "def get_fixtures_socres(season):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/schedule/{}-Premier-League-Scores-and-Fixtures\".format(season, season)\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first table\n",
        "  table = soup.find('table', {'class': 'stats_table'})\n",
        "  if table == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame\n",
        "  df = pd.read_html(str(table))[0]\n",
        "\n",
        "  filename = '/content/drive/MyDrive/APS360_Project/Data_2/{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "          df.to_excel(writer, sheet_name='Fixture data')\n",
        "\n",
        "\n",
        "def process_season(season):\n",
        "    get_fixtures_socres(season)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjmdnkqoPLYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZVxSDJdgQFZ"
      },
      "outputs": [],
      "source": [
        "def get_squad_data(season, stat_type,league):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/{}/{}-{}\".format(season,stat_type, season, league)\n",
        "\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first two tables\n",
        "  tables = soup.find_all('table', {'class': 'stats_table'})[:2]\n",
        "  if tables == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame for each table\n",
        "  dfs = [pd.read_html(str(table))[0] for table in tables]\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/EFL_{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  if not os.path.isfile(filename):\n",
        "    # Create a new file\n",
        "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "        dfs[0].to_excel(writer, sheet_name='Squad Data {}'.format(stat_type))\n",
        "  else:\n",
        "    with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "      dfs[0].to_excel(writer, sheet_name='Squad Data {}'.format(stat_type))\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_squad_data(season, stat_type,'Championship-Stats')\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UXWttr5YDr9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RCeyfLKU1rFZxxz8q-nKKpEaz5mUJ2wX",
      "authorship_tag": "ABX9TyOBTWcztLOG8F8yan4BENpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
