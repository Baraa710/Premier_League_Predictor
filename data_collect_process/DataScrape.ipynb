{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOf6QIxhQjuhFVDElK1Csb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baraa710/Premier_League_Predictor/blob/main/data_collect_process/DataScrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to use firefox webdriver in google colab\n",
        "\n",
        "!apt-get update\n",
        "!apt install firefox-esr\n",
        "!wget https://github.com/mozilla/geckodriver/releases/download/v0.31.0/geckodriver-v0.31.0-linux64.tar.gz\n",
        "!tar -xzvf geckodriver-v0.31.0-linux64.tar.gz\n",
        "!mv geckodriver /usr/local/bin\n",
        "!pip install selenium\n",
        "\n"
      ],
      "metadata": {
        "id": "1BQc1eNX3nK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58cd1d2-73ff-44fd-f897-ff7f95b19300"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connected to cloud.r-project.org (52.85.151.93)] [Connecting \r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package firefox-esr is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mPackage 'firefox-esr' has no installation candidate\u001b[0m\n",
            "--2024-03-10 23:24:28--  https://github.com/mozilla/geckodriver/releases/download/v0.31.0/geckodriver-v0.31.0-linux64.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/fe5af649-6466-4033-a53f-a449543fb73e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240310T232428Z&X-Amz-Expires=300&X-Amz-Signature=22bfe2174c9a00d753acb5cda544b4c6483f469c78857f7c85c41157ca9c6da2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.31.0-linux64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-03-10 23:24:28--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/fe5af649-6466-4033-a53f-a449543fb73e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240310T232428Z&X-Amz-Expires=300&X-Amz-Signature=22bfe2174c9a00d753acb5cda544b4c6483f469c78857f7c85c41157ca9c6da2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.31.0-linux64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2715370 (2.6M) [application/octet-stream]\n",
            "Saving to: ‘geckodriver-v0.31.0-linux64.tar.gz.1’\n",
            "\n",
            "geckodriver-v0.31.0 100%[===================>]   2.59M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-03-10 23:24:28 (107 MB/s) - ‘geckodriver-v0.31.0-linux64.tar.gz.1’ saved [2715370/2715370]\n",
            "\n",
            "geckodriver\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.18.1)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.24.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import multiprocessing\n",
        "import requests"
      ],
      "metadata": {
        "id": "myby2Arghm82"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "oHgOeeRTL1Cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99c4bc5-3caa-4fab-b58a-21edde6176ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ranges = [(n, n+1) for n in range(2017, 2024)]\n",
        "\n",
        "seasons = [\"{}-{}\".format(year1,year2) for (year1, year2) in ranges]\n",
        "stat_types = [('stats','stats_standard'), ('keepers', 'stats_keeper'), ('shooting','stats_shooting'), ('defense','stats_defense'), ('possession','stats_possession')]"
      ],
      "metadata": {
        "id": "k1EVKr5yYAW0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "eIvq_3qhmU7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567ff656-d329-4af0-c785-4e3fd2a3ab37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_database(season, stat_type, table_id):\n",
        "\n",
        "  driver.get(\"https://fbref.com/en/comps/9/{}/{}/{}-Premier-League-Stats\".format(season,stat_type, season))\n",
        "  # Wait for the page to load dynamically\n",
        "  driver.implicitly_wait(10)\n",
        "\n",
        "  # Get the page source\n",
        "  html_data = driver.page_source\n",
        "\n",
        "  # Parse the HTML data using BeautifulSoup\n",
        "  soup = BeautifulSoup(html_data, \"html.parser\")\n",
        "\n",
        "  # Find the table with id \"stats_standard\"\n",
        "  table = soup.find('table', {'id': table_id})\n",
        "  if table == None:\n",
        "    print(\"Could not find table for season {} stat type {}\".format(season, stat_type))\n",
        "    return\n",
        "\n",
        "  # Extract all rows from the table\n",
        "  rows = table.find_all('tr')\n",
        "  col_names = rows[1].find_all('th')\n",
        "  col_names = [ele.text.strip() for ele in col_names]\n",
        "  # Create an empty list to store the data\n",
        "  data = []\n",
        "\n",
        "  # Loop through the rows and extract the data from each row, skipping the first row\n",
        "  for row in rows[2:]:\n",
        "      cols = row.find_all('th') + row.find_all('td')\n",
        "      cols = [ele.text.strip() for ele in cols]\n",
        "      if(cols[0].isdigit()!=True):\n",
        "        continue\n",
        "      data.append(cols)  # Get rid of empty values\n",
        "\n",
        "  # Create a DataFrame from the data list\n",
        "  df = pd.DataFrame(data, columns = col_names)\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/{}.xlsx'.format(season)\n",
        "  if not os.path.isfile(filename):\n",
        "    # Create a new file\n",
        "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "        df.to_excel(writer, sheet_name=table_id)\n",
        "  else:\n",
        "    with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "      df.to_excel(writer, sheet_name=table_id)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_database(season, stat_type, table_id)\n",
        "\n",
        "# Initialize driver\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()\n",
        "# Quit the driver\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "Yzj4UOliUUjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to scrape first two tables from url into two excel sheets in the same xlsx file without selenium\n",
        "\n",
        "def get_squad_data(season, stat_type):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/{}/{}-Premier-League-Stats\".format(season,stat_type, season)\n",
        "\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first two tables\n",
        "  tables = soup.find_all('table', {'class': 'stats_table'})[:2]\n",
        "  if tables == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame for each table\n",
        "  dfs = [pd.read_html(str(table))[0] for table in tables]\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "          dfs[0].to_excel(writer, sheet_name='Squad Data {}'.format(stat_type))\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_squad_data(season, stat_type)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ],
      "metadata": {
        "id": "YtHbLd7HZNmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fixtures_socres(season):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/schedule/{}-Premier-League-Scores-and-Fixtures\".format(season, season)\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first table\n",
        "  table = soup.find('table', {'class': 'stats_table'})\n",
        "  if table == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame\n",
        "  df = pd.read_html(str(table))[0]\n",
        "\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "          df.to_excel(writer, sheet_name='Fixture data')\n",
        "\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_fixtures_socres(season)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z5KoHr3xai4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_squad_data(season, stat_type,league):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/{}/{}-{}\".format(season,stat_type, season, league)\n",
        "\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first two tables\n",
        "  tables = soup.find_all('table', {'class': 'stats_table'})[:2]\n",
        "  if tables == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame for each table\n",
        "  dfs = [pd.read_html(str(table))[0] for table in tables]\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/EFL_{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  if not os.path.isfile(filename):\n",
        "    # Create a new file\n",
        "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "        dfs[0].to_excel(writer, sheet_name='Squad Data {}'.format(stat_type))\n",
        "  else:\n",
        "    with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "      dfs[0].to_excel(writer, sheet_name='Squad Data {}'.format(stat_type))\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_squad_data(season, stat_type,'Championship-Stats')\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ],
      "metadata": {
        "id": "vZVxSDJdgQFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4UXWttr5YDr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}