{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTW4vv0X59m0pOy95Ey5UR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baraa710/Premier_League_Predictor/blob/main/data_collect_process/DataScrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to use firefox webdriver in google colab\n",
        "\n",
        "!apt-get update\n",
        "!apt install firefox-esr\n",
        "!wget https://github.com/mozilla/geckodriver/releases/download/v0.31.0/geckodriver-v0.31.0-linux64.tar.gz\n",
        "!tar -xzvf geckodriver-v0.31.0-linux64.tar.gz\n",
        "!mv geckodriver /usr/local/bin\n",
        "!pip install selenium\n",
        "\n"
      ],
      "metadata": {
        "id": "1BQc1eNX3nK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import multiprocessing\n",
        "import requests"
      ],
      "metadata": {
        "id": "myby2Arghm82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "oHgOeeRTL1Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranges = [(n, n+1) for n in range(2017, 2024)]\n",
        "\n",
        "seasons = [\"{}-{}\".format(year1,year2) for (year1, year2) in ranges]\n",
        "stat_types = [('stats','stats_standard'), ('keepers', 'stats_keeper'), ('shooting','stats_shooting'), ('defense','stats_defense'), ('possession','stats_possession')]"
      ],
      "metadata": {
        "id": "k1EVKr5yYAW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "eIvq_3qhmU7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_database(season, stat_type, table_id):\n",
        "\n",
        "  driver.get(\"https://fbref.com/en/comps/9/{}/{}/{}-Premier-League-Stats\".format(season,stat_type, season))\n",
        "  # Wait for the page to load dynamically\n",
        "  driver.implicitly_wait(10)\n",
        "\n",
        "  # Get the page source\n",
        "  html_data = driver.page_source\n",
        "\n",
        "  # Parse the HTML data using BeautifulSoup\n",
        "  soup = BeautifulSoup(html_data, \"html.parser\")\n",
        "\n",
        "  # Find the table with id \"stats_standard\"\n",
        "  table = soup.find('table', {'id': table_id})\n",
        "  if table == None:\n",
        "    print(\"Could not find table for season {} stat type {}\".format(season, stat_type))\n",
        "    return\n",
        "\n",
        "  # Extract all rows from the table\n",
        "  rows = table.find_all('tr')\n",
        "  col_names = rows[1].find_all('th')\n",
        "  col_names = [ele.text.strip() for ele in col_names]\n",
        "  # Create an empty list to store the data\n",
        "  data = []\n",
        "\n",
        "  # Loop through the rows and extract the data from each row, skipping the first row\n",
        "  for row in rows[2:]:\n",
        "      cols = row.find_all('th') + row.find_all('td')\n",
        "      cols = [ele.text.strip() for ele in cols]\n",
        "      if(cols[0].isdigit()!=True):\n",
        "        continue\n",
        "      data.append(cols)  # Get rid of empty values\n",
        "\n",
        "  # Create a DataFrame from the data list\n",
        "  df = pd.DataFrame(data, columns = col_names)\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/{}.xlsx'.format(season)\n",
        "  if not os.path.isfile(filename):\n",
        "    # Create a new file\n",
        "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "        df.to_excel(writer, sheet_name=table_id)\n",
        "  else:\n",
        "    with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "      df.to_excel(writer, sheet_name=table_id)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_database(season, stat_type, table_id)\n",
        "\n",
        "# Initialize driver\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()\n",
        "# Quit the driver\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "Yzj4UOliUUjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to scrape first two tables from url into two excel sheets in the same xlsx file without selenium\n",
        "\n",
        "def get_squad_data(season, stat_type):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/{}/{}-Premier-League-Stats\".format(season,stat_type, season)\n",
        "\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first two tables\n",
        "  tables = soup.find_all('table', {'class': 'stats_table'})[:2]\n",
        "  if tables == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame for each table\n",
        "  dfs = [pd.read_html(str(table))[0] for table in tables]\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "      # Write each DataFrame to a separate sheet\n",
        "      for i, df in enumerate(dfs):\n",
        "          df.to_excel(writer, sheet_name='Squad Data {}'.format(i + 1))\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_squad_data(season, stat_type)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ],
      "metadata": {
        "id": "YtHbLd7HZNmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fixtures_socres(season):\n",
        "# Define the URL to scrape\n",
        "  url = \"https://fbref.com/en/comps/9/{}/schedule/{}-Premier-League-Scores-and-Fixtures\".format(season, season)\n",
        "  # Fetch the HTML content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "  # Extract the first table\n",
        "  table = soup.find('table', {'class': 'stats_table'})\n",
        "  if table == None:\n",
        "    print(\"No table found\")\n",
        "    return\n",
        "  # Create a Pandas DataFrame\n",
        "  df = pd.read_html(str(table))[0]\n",
        "\n",
        "  filename = '/content/gdrive/MyDrive/APS360_Project/Data/{}.xlsx'.format(season)\n",
        "  # write to excel\n",
        "  with pd.ExcelWriter(filename, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
        "          df.to_excel(writer, sheet_name='Fixture data')\n",
        "\n",
        "\n",
        "def process_season(season):\n",
        "  for i,(stat_type, table_id) in enumerate(stat_types):\n",
        "    get_fixtures_socres(season)\n",
        "\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "pool = multiprocessing.Pool(processes=num_processes)\n",
        "pool.map(process_season, seasons)\n",
        "pool.close()\n",
        "pool.join()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z5KoHr3xai4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZVxSDJdgQFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}